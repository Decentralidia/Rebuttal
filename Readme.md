We thank you for your valuable comments on our submission “User Voices, Platform Choices: Social Media Policy Puzzle with Decentralization Salt”. We addressed your concerns in the following by iterating on your questions and providing corresponding response to them.

<div style="background-color: #FFFF00">1. Are the authors considering the use of additional metrics, beyond accuracy?</div>

Thank you for your feedback regarding the reported metrics. However, as indicated in `Section 6.2` of the paper (lines 617 and 618), we addressed the issue of imbalanced data in our test dataset by ensuring equal samples from both groups for the final evaluation. Consequently, we opted to focus solely on the accuracy metric, considering space constraints in the paper.

**`2. Have you checked Gobo https://www.media.mit.edu/projects/gobo/overview/? Are there similar projects these days?`**

We sincerely appreciate your valuable feedback and acknowledgment of similar projects. We thoroughly investigated the project you highlighted and commend the excellent work it represents, particularly in examining social media governance from a unique perspective—encouraging users to establish personal policies for their social media spaces.

In contrast, our project centers around the development of a decentralized policy for social media governance, leveraging community preferences alongside a machine learning-based module. While we did not identify entirely analogous projects, we acknowledge that the utilization of decentralization in policy-making is not unprecedented. In our case, we are applying this concept within the domain of social media platforms. Additionally, we have drawn inspiration from fundamental machine learning concepts and methods, incorporating them into our decentralized policy-making approach.

**`3. What are the policies about vetting shared by X? Are they following the shared policies?`**

That's a valid observation. Initially, in the early stages of our project definition, we considered requiring users to input reasons when voting on a tweet. However, as the project progressed, we recognized the inherent complexity for users, especially those encountering the experiment and platform for the first time, to provide reasons during the voting process. This realization prompted us to reevaluate our approach and make adjustments to ensure a more user-friendly and intuitive experience.

**`4. What are the policies about vetting shared by X? Are they following the shared policies?`**

Thank you for providing a detailed overview of the tweet and tweet rating process. In `Figure (1)`, we have included a screenshot of the questionnaire and the associated platform to offer a comprehensive understanding of the experiment conditions.

To mitigate the potential for bias, particularly toward X, we deliberately opted not to display X's viewpoint and stance on the tweet. Our emphasis on maintaining an even distribution of respondents throughout the experiment led us to select participants from both the informed and uninformed segments of the X policies audience. It's important to note that the entire process, including responding to tweets, was conducted anonymously.

Initially, our project envisioned compelling users to provide reasons when voting on a tweet. However, as the experiment progressed and users encountered the platform for the first time, we recognized the complexity involved in asking participants to articulate their reasons. Consequently, we decided to streamline the user experience by removing the requirement for reasons.

Throughout the experiment, we placed a high priority on fostering unbiased and self-formed opinions among respondents. Although the idea of posing questions such as "X suggests that this post should be vetted because _____ Do you agree or disagree with this statement?" seemed intriguing, we opted against such inquiries to minimize the potential for bias in the respondents' final opinions.
